************************************
Парсинг удаленного источника (сайта)
************************************

Здесь мы рассмотрим пример парсинга интернет-сайта. Одним из наиболее популярных объектов парсинга являются сайты относящиеся к одному из типов

* интернет-магазины

* сайты объявлений купли-продажи

* сайты риэлтерских агентств

Все они хранят большой объем данных соответствующей предметной области. А задача парсинга - извлечь эти данные и сохранить в удобном для последующего поиска и анализа виде. Адреса некоторых таких сайтов можно узнать здесь ___

Поскольку в предыдущем примере мы извлекали данные из общероссийской базы вакансий, продолжим данную тематику и рассмотрим пример извлечения данных из сайта популярного рекрутинкового агенства HeadHunter ___

Перед разбором примера следует заметить, что на рассматриваемом сайте информацию о вакансиях можно извлекать минимум тремя способами

1. используя API сайта

2. используя rss сайта

3. непосредственно со страниц сайта 

Однако, не все сайты предоставляют такое разноообразие способов для извлечения данных. Поэтому, в данном примере мы рассмотрим наиболее универсальный (хотя, зачастую, и не самый удобный) способ под номером 3 - извлечение данных непосредственно со страниц сайта

Следует понимать, что процесс извлечения данных может быть реализован несколькими путями. Чтобы выбрать наиболее эффективный путь (с точки зрения времени и трудозатрат) первым и самым важным этапом является анализ структуры сайта и страниц, хранящих необходимую информацию. 

Этап 1. Анализ сайта
--------------------
Анализ - творческий процесс, во многом зависит структурой сайта и определяет последующий способ его парсинга.  

Сбор информации о ссылка сайта, хранящих необходимые данные
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Сбор информации может выполняться одним из двух способов

Автоматический способ - самый простой но более длительный и подходит не для всех типов сайтов. Суть его заключается в том, чтобы дать парсеру указание обойти все доступные страницы сайта, извлечь из них все внутренние ссылки сайта и сохранить их в отдельный файл. Делается это следующей командой

__

Затем нужно просмотреть файл со собранными ссылками и выделить те из них, по которым хранятся необходимые данные. Например

__

Из листинга видно, что достаточно много ссылок имеют такую структуру ___

Перейдя по любой из них в браузере мы попадаем на страницу, содержащую нужные нам данные. Значит, именно эти страницы и нужно указать функции парсера для извлечения данных. Делается это примерно так

__ (парсинг из файла по шаблону страницы

Способ второй - ручной. Переходя в браузере по ссылкам сайта вручную определить ссылки какого типа нам необходимы. Например ___

Очевидно, что для данного способа нам нет нужны собирать все ссылки с сайта - мы можем выполнять парсинг генерируя их "на лету". Генерировать ссылки можно, например таким кодом

.. code-block:: python

    for j in ['http://hh.%(domen)s/data/%(id)s?filter1=%(f)s&filter2=%(ff)s' % {'id':str(i), 'domen':d, 'f':f, 'ff':ff}
		for d in ['com', 'ru'] 
		  for i in xrange(1,10,1)
			for f in ['profession','employers']
			  for ff in xrange(5)
		]:
		print j



Первая особенность интернет сайтов как источника xml данных заключается в том, что данные хранятся по адресам (страниц) небольшими порциями. И таких адресов у сайта очень много. А кроме того, сайт включает адреса к другим данным, нас не интересующим. В нашем примере это адреса на описание компаний, разместивших вакансии, адреса на страницы ___ и т.д. Нас интересуют только адреса с описанием вакансий.

Поэтому, задача данного этапа - собрать все адреса сайта и оставить только те, по которым хранятся интересующие нас данные и.

Решается эта задача с помощью следующего кода

.. code-block:: python

	url='http://www.hh.ru'
	x=xmlParser(url, 'html')
	x.header={}
	x.html.getAllLinks(link_file='hh.lst', maxdeep=-1)
	
Код сканирует сайт и сохраняет все адреса в файл hh.lst

:maxdeep: важный параметр метода - определяет глубину поиска ссылок. Т.е. сначала из первой (основной) страницы сайта извлекаются все ссылки на другие страницы. Затем метод переходит по каждой этой ссылке и уже из этой страницы извлекает все ссылкию Затем переходит по ним и опять извлекает все ссылки. Т.е. проходит вглубь сайта до уровня **maxdeep**. Значение -1 означает что ссылки будут извлекаться со всех доступных страниц сайта.

Задача данного метода - собрать достаточное количество ссылок, чтобы понять структуру сайта. Поэтому иногда не обязательно собирать все ссылки, а ограничиться глубиной 5-10

Этап 2. Анализ ссылок
---------------------
Теперь необходимо вручную провести небольшой анализ ссылок. Дело в том, что нужные нам одни и те же данные могут быть представлены на сайте в разных формах и по разным ссылкам (путям). Для нашего примера это выглядит так.
Мы собираем данные о вакансиях, размещенных на сайте. Вакансии представлены в следующих форматах

1. Самая подробная информация информация о вакансии хранится на страницах сайта примерно такого формата https://hh.ru/vacancy/16718241. По таким ссылкам на каждой странице хранятся данные только по одной вакансии

2. Список вакансий хранится по ссылкам такого типа https://hh.ru/search/vacancy?page=3. Здесь на каждой странице хранится сокращенная информация по 20 вакансиям. Но есть ограничение - максимально допустимое значение параметра -99. Т.о. меняя только этот параметр можно получить данные о 2000 вакансиях. Чтобы получить больше данных - на помощь приходят отфильтрованные списки.


3. Отфильтронаннык списки вакансий хранятся по тому же адресу что и в 2 https://hh.ru/search/vacancy?page=3, но с добавлением к адресу дополнительных фильтров, например https://hh.ru/search/vacancy?page=3&area=1 - вакансии по г. Москва. Просматривая ссылки - легко выделить следуюште параметры фильтров

*areaId* - номер региона в котором размещена вакансия (__ уникальных значений)

*only_with_salary=true* - вакансии только с указанной зарплатой

*salary* - нижняя граница заработной платы

*experience=between1And3* - опыт работы

*specialization=21* - профессиональная область и специализации

*industry=5* - отрасль кампании

*label=not_from_agency* - исключить вакансии агенств

*employment* - тип занятости

*schedule* - график работы

Каждый параметр имеет ограниченное количество допустимых значений. Узнать их можно из списка ссылок. для этого вызовем метод


4. Список вакансий по профессиям хранятся по адресу https://hh.ru/catalog/Avtomobilnyj-biznes/Nachalnii-uroven/page-2

5. Список краткого описания вакансий по кампаниям хранится по адресу https://hh.ru/employer/5406

Такое разнообразие представления информации ___

* Если мы хотим собрать наиболее полную базу вакансий для ___

* если мы хотим проанализировать рынок вакансий в сравнении с общероссийской базой - лучше всего использовать 3-й тип ссылок

* если мы хотим регулярно отслеживать поступления новых вакансий по перечню определенных профессий или по определенным кампаниям, например, чтобы при появлении таких вакансий автоматически отправлять отчет на электронную почту, соотвественно используем 4 или 5 тип ссылок 

Далее в данном примере мы будем рассматривать задачу сравнения вакансий hh с общероссийской базой

Этап 3. Извлечение данных по нужным ссылкам
-------------------------------------------

На данном этапе существует два подхода к извлечению данных. Точнее - к получению ссылок на страницы из которых парсер будет извлекать данные

1. Указать шаблон для ссылок по которым хранятся необходимые нам данные

2. Генерировать необходимые ссылки.

Рассмотрим подробно оба этих способа. Ни один из них не является преимущественным, применение того или иного способа зависит организации сайта, от времени, которым Вы располагается для извлечения данных и навыков программирования.

Способ 1. Определение шаблона ссылок
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
не работает


Способ 2. Генерация ссылок
~~~~~~~~~~~~~~~~~~~~~~~~~~
Данный способ позволяет вообще пропустить этап 2 (сбор ссылок сайта) но при этом
основан на предварительном ручном изучении существующих фильтров и их допустимых значений (см пункт 3 этапа 2). После этого в коре парсинга необходимо сформировать словарь фильтров и их допустимых значений, после чего можно запустить парсер для извлечения данных по генерируемым ссылкам. код будет примерно такой

__ 
 
 
___ проверить оба способа и сравнить их точность ___         
 