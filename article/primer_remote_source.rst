************************************
Парсинг удаленного источника (сайта)
************************************

Здесь мы рассмотрим пример парсинга сайта в интренет. Одним из наиболее популярных объектов парсинга являются сайты типа "интернет магазин". Целью парсинга таких сайтов - извлечение данных о товарах - наименование, описание, цена, изображение. Адреса некоторых популярных интернет-магазинов можно узнать здесь ___

Однако, поскольку в предыдущем примере мы извлекали данные из общероссийской базы вакансий, продолжим данную тематику и рассмотрим пример извлечения данных из сайта популярного рекрутинкового агенства HeadHunter ___

Перед разбором примера следует заметить, что на рассматриваемом сайте информацию о вакансиях можно извлекать минимум тремя способами

1. используя API сайта

2. используя rss сайта

3. непосредственно со страниц сайта 

Однако, не все сайты предоставляют такое разноообразие способов для извлечения данных. Поэтому, в данном примере мы рассмотрим наиболее универсальный (хотя и не самый удобный) способ под номером 3 - извлечение данных непосредственно со страниц сайта

Этап 1. Сбор ссылок на страницы сайта
----------------------------------------------

Первая особенность интернет сайтов как источника xml данных заключается в том, что данные хранятся по адресам (страниц) небольшими порциями. И таких адресов у сайта очень много. А кроме того, сайт включает адреса к другим данным, нас не интересующим. В нашем примере это адреса на описание компаниц, разместивших вакансии, адреса на страницы ___ и т.д. Нас интересуют только адреса с описанием вакансий.

Поэтому, задача данного этапа - собрать все адреса сайта и оставить только те, по которым хранятся интересующие нас данные и.

Решается эта задача с помощью следующего кода

.. code-block:: python

	url='http://www.hh.ru'
	x=xmlParser(url, 'html')
	x.header={}
	x.html.getAllLinks(link_file='hh.lst', maxdeep=-1)
	
Код сканирует сайт и сохраняет все адреса в файл hh.lst

:maxdeep: важный параметр метода - определяет глубину поиска ссылок. Т.е. сначала из первой (основной) страницы сайта извлекаются все ссылки на другие страницы. Затем метод переходит по каждой этой ссылке и уже из этой страницы извлекает все ссылкию Затем переходит по ним и опять извлекает все ссылки. Т.е. проходит вглубь сайта до уровня **maxdeep**. Значение -1 означает что ссылки будут извлекаться со всех доступных страниц сайта.

Задача данного метода - собрать достаточное количество ссылок, чтобы понять структуру сайта. Поэтому иногда не обязательно собирать все ссылки, а ограничиться глубиной 5-10

Этап 2. Анализ ссылок
---------------------
Теперь необходимо вручную провести небольшой анализ ссылок. Дело в том, что нужные нам одни и те же данные могут быть представлены на сайте в разных формах и по разным ссылкам (путям). Для нашего примера это выглядит так.
Мы собираем данные о вакансиях, размещенных на сайте. Вакансии представлены в следующих форматах

1. Самая подробная информация информация о вакансии хранится на страницах сайта примерно такого формата https://hh.ru/vacancy/16718241. По таким ссылкам на каждой странице хранятся данные только по одной вакансии

2. Список вакансий хранится по ссылкам такого типа https://hh.ru/search/vacancy?page=3. Здесь на каждой странице хранится сокращенная информация по 20 вакансиям. Но есть ограничение - максимально допустимое значение параметра -99. Т.о. меняя только этот параметр можно получить данные о 2000 вакансиях. Чтобы получить больше данных - на помощь приходят отфильтрованные списки.


3. Отфильтронаннык списки вакансий хранятся по тому же адресу что и в 2 https://hh.ru/search/vacancy?page=3, но с добавлением к адресу дополнительных фильтров, например https://hh.ru/search/vacancy?page=3&area=1 - вакансии по г. Москва. Просматривая ссылки - легко выделить следуюште параметры фильтров

*areaId* - номер региона в котором размещена вакансия (__ уникальных значений)

*only_with_salary=true* - вакансии только с указанной зарплатой

*salary* - нижняя граница заработной платы

*experience=between1And3* - опыт работы

*specialization=21* - профессиональная область и специализации

*industry=5* - отрасль кампании

*label=not_from_agency* - исключить вакансии агенств

*employment* - тип занятости

*schedule* - график работы

Каждый параметр имеет ограниченное количество допустимых значений. Узнать их можно из списка ссылок. для этого вызовем метод


4. Список вакансий по профессиям хранятся по адресу https://hh.ru/catalog/Avtomobilnyj-biznes/Nachalnii-uroven/page-2

5. Список краткого описания вакансий по кампаниям хранится по адресу https://hh.ru/employer/5406

Такое разнообразие представления информации ___

* Если мы хотим собрать наиболее полную базу вакансий для ___

* если мы хотим проанализировать рынок вакансий в сравнении с общероссийской базой - лучше всего использовать 3-й тип ссылок

* если мы хотим регулярно отслеживать поступления новых вакансий по перечню определенных профессий или по определенным кампаниям, например, чтобы при появлении таких вакансий автоматически отправлять отчет на электронную почту, соотвественно используем 4 или 5 тип ссылок 

Далее в данном примере мы будем рассматривать задачу сравнения вакансий hh с общероссийской базой
 
 
         
 